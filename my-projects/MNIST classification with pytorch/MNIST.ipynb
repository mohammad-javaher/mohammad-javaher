{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gSQEo9KEHZJX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GDJSbru_H_rL"
      },
      "outputs": [],
      "source": [
        "# Device configuration (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPJTPOyKR8W"
      },
      "source": [
        "# Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7dNW4mIrIPYP"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAE3wM0QKVTh"
      },
      "source": [
        "# MNIST dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw_N2IxzIlNn",
        "outputId": "ef7409fa-4f86-4b12-d7d9-f87bceffb32f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.07MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.51MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.50MB/s]\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./root',\n",
        "                                           train=True,\n",
        "                                           transform=transform,\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./root',\n",
        "                                           train=False,\n",
        "                                           transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_RVXRqgMKjZ"
      },
      "source": [
        "# Simple Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "eOKoK3UfI7RM"
      },
      "outputs": [],
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # flatten\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleNN().to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez1cCyBEPEKD"
      },
      "source": [
        "# Training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4va1gWl0OzbQ",
        "outputId": "9dbfc9f2-9c8e-4ea1-ee6b-ea09d30a010b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/15], Loss: 0.3944\n",
            "Epoch [2/15], Loss: 0.0836\n",
            "Epoch [3/15], Loss: 0.0482\n",
            "Epoch [4/15], Loss: 0.0218\n",
            "Epoch [5/15], Loss: 0.0047\n",
            "Epoch [6/15], Loss: 0.0256\n",
            "Epoch [7/15], Loss: 0.0109\n",
            "Epoch [8/15], Loss: 0.0149\n",
            "Epoch [9/15], Loss: 0.0026\n",
            "Epoch [10/15], Loss: 0.0079\n",
            "Epoch [11/15], Loss: 0.0004\n",
            "Epoch [12/15], Loss: 0.0078\n",
            "Epoch [13/15], Loss: 0.0004\n",
            "Epoch [14/15], Loss: 0.0001\n",
            "Epoch [15/15], Loss: 0.0025\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-huYyrbLSF-q"
      },
      "source": [
        "# Testing loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGfMMX7MSGrM",
        "outputId": "9f8000d3-13fc-451b-b7ae-af086345eebb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.79%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvYzLT75V80Y"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "EPUkfDPsWbZf",
        "outputId": "40a9f835-493e-4162-84cc-f4de8cd1c0ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload an image of a single handwritten digit.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-93222972-1138-47db-8b4b-f1ebc8144772\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-93222972-1138-47db-8b4b-f1ebc8144772\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1755704726568.png to 1755704726568.png\n",
            "\n",
            "--- Prediction ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "# --- Helper Function to Preprocess the Image ---\n",
        "def preprocess_image(image_data):\n",
        "    \"\"\"\n",
        "    Prepares the uploaded image to be compatible with the MNIST model.\n",
        "    - Converts to grayscale\n",
        "    - Inverts colors (model expects white digit on black background)\n",
        "    - Resizes to 28x28 pixels\n",
        "    - Converts to a PyTorch tensor and adds batch dimension\n",
        "    \"\"\"\n",
        "    # Open image and convert to grayscale ('L' mode)\n",
        "    image = Image.open(io.BytesIO(image_data)).convert('L')\n",
        "\n",
        "    # MNIST digits are white on a black background. If your image is\n",
        "    # black on a white background, we need to invert it.\n",
        "    image = ImageOps.invert(image)\n",
        "\n",
        "    # Resize to 28x28, the size the model was trained on\n",
        "    image = image.resize((28, 28))\n",
        "\n",
        "    # Apply the same ToTensor transformation\n",
        "    transform = transforms.ToTensor()\n",
        "    # Add an extra dimension for the batch and send to the correct device\n",
        "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Return the processed tensor and the image for display\n",
        "    return image_tensor, image\n",
        "\n",
        "\n",
        "# --- Main Prediction Logic ---\n",
        "\n",
        "# 1. Display an upload button\n",
        "print(\"Please upload an image of a single handwritten digit.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Check if a file was uploaded\n",
        "if len(uploaded) == 0:\n",
        "    print(\"No file was uploaded.\")\n",
        "else:\n",
        "    # Get the first uploaded file's data\n",
        "    filename = next(iter(uploaded))\n",
        "    img_data = uploaded[filename]\n",
        "\n",
        "    # 2. Preprocess the image for the model\n",
        "    image_tensor, display_image = preprocess_image(img_data)\n",
        "\n",
        "    # 3. Make a prediction\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Turn off gradients for inference\n",
        "        output = model(image_tensor)\n",
        "        # Get the class with the highest score\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        prediction = predicted.item()\n",
        "\n",
        "    # 4. Display the image and the prediction\n",
        "    print(\"\\n--- Prediction ---\")\n",
        "    plt.imshow(display_image, cmap='gray')\n",
        "    plt.title(f\"The model predicts this is a: {prediction}\", fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}